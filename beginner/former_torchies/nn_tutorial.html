

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nn package &mdash; PyTorch Tutorials 0.4.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/pytorch_theme.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Multi-GPU examples" href="parallelism_tutorial.html" />
    <link rel="prev" title="Autograd" href="autograd_tutorial.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>
  <!-- <link href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i" rel="stylesheet"> -->
</head>

<div class="pytorch-header">
  <div class="pytorch-container">
    <div class="pytorch-header-logo">
      <img src="../../_static/pytorch-logo-dark.svg" class="logo" alt="Logo"/>
    </div>

    <div class="pytorch-main-menu">
      <ul>
        <li><a href="">Get Started</a></li>
        <li><a href="">Features</a></li>
        <li><a href="">Ecosystem</a></li>
        <li><a href="">Blog</a></li>
        <li><a href="" class="active">Tutorials</a></li>
        <li><a href="">Docs</a></li>
        <li><a href="">Resources</a></li>
        <li><a href="">Github</a></li>
      </ul>
    </div>
  </div>
</div>

<body class="pytorch-body">

   
  <div>

    
    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-left-menu-search">
          

          
            
            
              <div class="version">
                0.4.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Beginner Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../former_torchies_tutorial.html">PyTorch for former Torch users</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tensor_tutorial.html">Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd_tutorial.html">Autograd</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">nn package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example-1-convnet">Example 1: ConvNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="#forward-and-backward-function-hooks">Forward and Backward Function Hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-2-recurrent-net">Example 2: Recurrent Net</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="parallelism_tutorial.html">Multi-GPU examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_with_examples.html">Learning PyTorch with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../transfer_learning_tutorial.html">Transfer Learning tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_loading_tutorial.html">Data Loading and Processing Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning_nlp_tutorial.html">Deep Learning for NLP with Pytorch</a></li>
</ul>
<p class="caption"><span class="caption-text">Intermediate Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html">Classifying Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html">Generating Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html">Translation with a Sequence to Sequence Network and Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html">Reinforcement Learning (DQN) tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dist_tuto.html">Writing Distributed Applications with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/neural_style_tutorial.html">Neural Transfer with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/numpy_extensions_tutorial.html">Creating extensions using numpy and scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/super_resolution_with_caffe2.html">Transfering a model from PyTorch to Caffe2 and Mobile using ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../former_torchies_tutorial.html">PyTorch for former Torch users</a> &raquo;</li>
        
      <li>nn package</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../_sources/beginner/former_torchies/nn_tutorial.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
</div>
          <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
           <article itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Click <a class="reference internal" href="#sphx-glr-download-beginner-former-torchies-nn-tutorial-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="nn-package">
<span id="sphx-glr-beginner-former-torchies-nn-tutorial-py"></span><h1>nn package<a class="headerlink" href="#nn-package" title="Permalink to this headline">¶</a></h1>
<p>We’ve redesigned the nn package, so that it’s fully integrated with
autograd. Let’s review the changes.</p>
<p><strong>Replace containers with autograd:</strong></p>
<blockquote>
<div><p>You no longer have to use Containers like <code class="docutils literal notranslate"><span class="pre">ConcatTable</span></code>, or modules like
<code class="docutils literal notranslate"><span class="pre">CAddTable</span></code>, or use and debug with nngraph. We will seamlessly use
autograd to define our neural networks. For example,</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">=</span> <span class="pre">nn.CAddTable():forward({input1,</span> <span class="pre">input2})</span></code> simply becomes
<code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">=</span> <span class="pre">input1</span> <span class="pre">+</span> <span class="pre">input2</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">=</span> <span class="pre">nn.MulConstant(0.5):forward(input)</span></code> simply becomes
<code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">=</span> <span class="pre">input</span> <span class="pre">*</span> <span class="pre">0.5</span></code></li>
</ul>
</div></blockquote>
<p><strong>State is no longer held in the module, but in the network graph:</strong></p>
<blockquote>
<div><p>Using recurrent networks should be simpler because of this reason. If
you want to create a recurrent network, simply use the same Linear layer
multiple times, without having to think about sharing weights.</p>
<div class="figure" id="id1">
<img alt="torch-nn-vs-pytorch-nn" src="../../_images/torch-nn-vs-pytorch-nn.png" />
<p class="caption"><span class="caption-text">torch-nn-vs-pytorch-nn</span></p>
</div>
</div></blockquote>
<p><strong>Simplified debugging:</strong></p>
<blockquote>
<div>Debugging is intuitive using Python’s pdb debugger, and <strong>the debugger
and stack traces stop at exactly where an error occurred.</strong> What you see
is what you get.</div></blockquote>
<div class="section" id="example-1-convnet">
<h2>Example 1: ConvNet<a class="headerlink" href="#example-1-convnet" title="Permalink to this headline">¶</a></h2>
<p>Let’s see how to create a small ConvNet.</p>
<p>All of your networks are derived from the base class <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>:</p>
<ul class="simple">
<li>In the constructor, you declare all the layers you want to use.</li>
<li>In the forward function, you define how your model is going to be
run, from input to output</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>


<span class="k">class</span> <span class="nc">MNISTConvNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># this is the place where you instantiate all your modules</span>
        <span class="c1"># you can later access them using the same names you&#39;ve given them in</span>
        <span class="c1"># here</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MNISTConvNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">320</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="c1"># it&#39;s the forward function that defines the network structure</span>
    <span class="c1"># we&#39;re accepting only a single input in here, but if you want,</span>
    <span class="c1"># feel free to use more</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="nb">input</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

        <span class="c1"># in your model definition you can go full crazy and use arbitrary</span>
        <span class="c1"># python code to define your model structure</span>
        <span class="c1"># all these are perfectly legal, and will be handled correctly</span>
        <span class="c1"># by autograd:</span>
        <span class="c1"># if x.gt(0) &gt; x.numel() / 2:</span>
        <span class="c1">#      ...</span>
        <span class="c1">#</span>
        <span class="c1"># you can even do a loop and reuse the same module inside it</span>
        <span class="c1"># modules no longer hold ephemeral state, so you can use them</span>
        <span class="c1"># multiple times during your forward pass</span>
        <span class="c1"># while x.norm(2) &lt; 10:</span>
        <span class="c1">#    x = self.conv1(x)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>Let’s use the defined ConvNet now.
You create an instance of the class first.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">MNISTConvNet</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>MNISTConvNet(
  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fc1): Linear(in_features=320, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=10, bias=True)
)
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> only supports mini-batches The entire <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>
package only supports inputs that are a mini-batch of samples, and not
a single sample.</p>
<p>For example, <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> will take in a 4D Tensor of
<code class="docutils literal notranslate"><span class="pre">nSamples</span> <span class="pre">x</span> <span class="pre">nChannels</span> <span class="pre">x</span> <span class="pre">Height</span> <span class="pre">x</span> <span class="pre">Width</span></code>.</p>
<p class="last">If you have a single sample, just use <code class="docutils literal notranslate"><span class="pre">input.unsqueeze(0)</span></code> to add
a fake batch dimension.</p>
</div>
<p>Create a mini-batch containing a single sample of random data and send the
sample through the ConvNet.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 10])
</pre></div>
</div>
<p>Define a dummy target label and compute error using a loss function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  <span class="c1"># LogSoftmax + ClassNLL Loss</span>
<span class="n">err</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">err</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(2.3546, grad_fn=&lt;NllLossBackward&gt;)
</pre></div>
</div>
<p>The output of the ConvNet <code class="docutils literal notranslate"><span class="pre">out</span></code> is a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>. We compute the loss
using that, and that results in <code class="docutils literal notranslate"><span class="pre">err</span></code> which is also a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.
Calling <code class="docutils literal notranslate"><span class="pre">.backward</span></code> on <code class="docutils literal notranslate"><span class="pre">err</span></code> hence will propagate gradients all the
way through the ConvNet to it’s weights</p>
<p>Let’s access individual layer weights and gradients:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([10, 1, 5, 5])
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">norm</span><span class="p">())</span>  <span class="c1"># norm of the weight</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">norm</span><span class="p">())</span>  <span class="c1"># norm of the gradients</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(1.8251)
tensor(0.1321)
</pre></div>
</div>
</div>
<div class="section" id="forward-and-backward-function-hooks">
<h2>Forward and Backward Function Hooks<a class="headerlink" href="#forward-and-backward-function-hooks" title="Permalink to this headline">¶</a></h2>
<p>We’ve inspected the weights and the gradients. But how about inspecting
/ modifying the output and grad_output of a layer?</p>
<p>We introduce <strong>hooks</strong> for this purpose.</p>
<p>You can register a function on a <code class="docutils literal notranslate"><span class="pre">Module</span></code> or a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.
The hook can be a forward hook or a backward hook.
The forward hook will be executed when a forward call is executed.
The backward hook will be executed in the backward phase.
Let’s look at an example.</p>
<p>We register a forward hook on conv2 and print some information</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">printnorm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
    <span class="c1"># input is a tuple of packed inputs</span>
    <span class="c1"># output is a Tensor. output.data is the Tensor we are interested</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Inside &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39; forward&#39;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;input: &#39;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;input[0]: &#39;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;output: &#39;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;input size:&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;output size:&#39;</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;output norm:&#39;</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">norm</span><span class="p">())</span>


<span class="n">net</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">printnorm</span><span class="p">)</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Inside Conv2d forward

input:  &lt;class &#39;tuple&#39;&gt;
input[0]:  &lt;class &#39;torch.Tensor&#39;&gt;
output:  &lt;class &#39;torch.Tensor&#39;&gt;

input size: torch.Size([1, 10, 12, 12])
output size: torch.Size([1, 20, 8, 8])
output norm: tensor(12.2174)
</pre></div>
</div>
<p>We register a backward hook on conv2 and print some information</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">printgradnorm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Inside &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39; backward&#39;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Inside class:&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;grad_input: &#39;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">grad_input</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;grad_input[0]: &#39;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">grad_input</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;grad_output: &#39;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">grad_output</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;grad_output[0]: &#39;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">grad_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;grad_input size:&#39;</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;grad_output size:&#39;</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;grad_input norm:&#39;</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">norm</span><span class="p">())</span>


<span class="n">net</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">register_backward_hook</span><span class="p">(</span><span class="n">printgradnorm</span><span class="p">)</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">err</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">err</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Inside Conv2d forward

input:  &lt;class &#39;tuple&#39;&gt;
input[0]:  &lt;class &#39;torch.Tensor&#39;&gt;
output:  &lt;class &#39;torch.Tensor&#39;&gt;

input size: torch.Size([1, 10, 12, 12])
output size: torch.Size([1, 20, 8, 8])
output norm: tensor(12.2174)
Inside Conv2d backward
Inside class:Conv2d

grad_input:  &lt;class &#39;tuple&#39;&gt;
grad_input[0]:  &lt;class &#39;torch.Tensor&#39;&gt;
grad_output:  &lt;class &#39;tuple&#39;&gt;
grad_output[0]:  &lt;class &#39;torch.Tensor&#39;&gt;

grad_input size: torch.Size([1, 10, 12, 12])
grad_output size: torch.Size([1, 20, 8, 8])
grad_input norm: tensor(0.0249)
</pre></div>
</div>
<p>A full and working MNIST example is located here
<a class="reference external" href="https://github.com/pytorch/examples/tree/master/mnist">https://github.com/pytorch/examples/tree/master/mnist</a></p>
</div>
<div class="section" id="example-2-recurrent-net">
<h2>Example 2: Recurrent Net<a class="headerlink" href="#example-2-recurrent-net" title="Permalink to this headline">¶</a></h2>
<p>Next, let’s look at building recurrent nets with PyTorch.</p>
<p>Since the state of the network is held in the graph and not in the
layers, you can simply create an nn.Linear and reuse it over and over
again for the recurrence.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="c1"># you can also accept arguments in your model constructor</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">data_size</span> <span class="o">+</span> <span class="n">hidden_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h2o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">data</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h2o</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">output</span>


<span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>A more complete Language Modeling example using LSTMs and Penn Tree-bank
is located
<a class="reference external" href="https://github.com/pytorch/examples/tree/master/word_language_model">here</a></p>
<p>PyTorch by default has seamless CuDNN integration for ConvNets and
Recurrent Nets</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">TIMESTEPS</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Create some fake data</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">TIMESTEPS</span><span class="p">):</span>
    <span class="c1"># yes! you can reuse the same network several times,</span>
    <span class="c1"># sum up the losses, and call backward!</span>
    <span class="n">hidden</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Total running time of the script:</strong> ( 0 minutes  0.013 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-former-torchies-nn-tutorial-py">
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../../_downloads/nn_tutorial.py" download=""><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">nn_tutorial.py</span></code></a></div>
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../../_downloads/nn_tutorial.ipynb" download=""><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">nn_tutorial.ipynb</span></code></a></div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.readthedocs.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


           </article>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="parallelism_tutorial.html" class="btn btn-neutral float-right" title="Multi-GPU examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="autograd_tutorial.html" class="btn btn-neutral" title="Autograd" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, PyTorch.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

      <div class="pytorch-content-right">
        <div class="pytorch-right-menu">
        </div>
      </div>
    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'0.4.1',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 

<script type="text/javascript">
  $(document).ready(function() {
    console.log('Testing...');
  });
</script>


</body>
</html>